{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Main.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/selveszero/530_final_project/blob/master/Main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "57SeeY0hovCV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir data\n",
        "!wget http://msvocds.blob.core.windows.net/annotations-1-0-3/captions_train-val2014.zip -P ./data/\n",
        "!wget http://images.cocodataset.org/zips/train2014.zip -P ./data/\n",
        "!wget http://images.cocodataset.org/zips/val2014.zip -P ./data/\n",
        "\n",
        "!unzip -q ./data/captions_train-val2014.zip -d ./data/\n",
        "!rm ./data/captions_train-val2014.zip\n",
        "!unzip -q ./data/train2014.zip -d ./data/\n",
        "!rm ./data/train2014.zip \n",
        "!unzip -q ./data/val2014.zip -d ./data/ \n",
        "!rm ./data/val2014.zip "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a4aE_UfatmSw",
        "colab_type": "code",
        "outputId": "5041f644-c274-4238-926b-1c994dda0096",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "uq-9SEFkn7Ea",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python build_vocab.py   \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fnw11LFboGdL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python resize.py "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wIKajPCFXzIL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The following cell trains 1 epoch of the training data.\n",
        "More tunable parameters can be seen in train.py"
      ]
    },
    {
      "metadata": {
        "id": "1yrIUDjbt6lm",
        "colab_type": "code",
        "outputId": "46b6e8cc-5d36-43a8-9ff8-5eae6db147b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        }
      },
      "cell_type": "code",
      "source": [
        "!python train.py --num_epochs=1 --model_path='vgg_models'"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(batch_size=128, caption_path='data/annotations/captions_train2014.json', crop_size=224, embed_size=256, hidden_size=512, image_dir='data/resized2014', learning_rate=0.001, log_step=100, model_path='vgg_models', num_epochs=1, num_layers=1, num_workers=2, save_step=1000, vocab_path='data/vocab.pkl')\n",
            "loading annotations into memory...\n",
            "Done (t=0.99s)\n",
            "creating index...\n",
            "index created!\n",
            "Epoch [0/1], Step [0/3236], Loss: 9.2146, Perplexity: 10042.8652\n",
            "Epoch [0/1], Step [100/3236], Loss: 3.8116, Perplexity: 45.2245\n",
            "Epoch [0/1], Step [200/3236], Loss: 3.4982, Perplexity: 33.0559\n",
            "Epoch [0/1], Step [300/3236], Loss: 3.2787, Perplexity: 26.5406\n",
            "Epoch [0/1], Step [400/3236], Loss: 3.0367, Perplexity: 20.8370\n",
            "Epoch [0/1], Step [500/3236], Loss: 2.8681, Perplexity: 17.6043\n",
            "Epoch [0/1], Step [600/3236], Loss: 2.8886, Perplexity: 17.9688\n",
            "Epoch [0/1], Step [700/3236], Loss: 2.9195, Perplexity: 18.5321\n",
            "Epoch [0/1], Step [800/3236], Loss: 2.9065, Perplexity: 18.2925\n",
            "Epoch [0/1], Step [900/3236], Loss: 2.6152, Perplexity: 13.6706\n",
            "Epoch [0/1], Step [1000/3236], Loss: 2.6411, Perplexity: 14.0280\n",
            "Epoch [0/1], Step [1100/3236], Loss: 2.5746, Perplexity: 13.1259\n",
            "Epoch [0/1], Step [1200/3236], Loss: 2.6258, Perplexity: 13.8154\n",
            "Epoch [0/1], Step [1300/3236], Loss: 2.4830, Perplexity: 11.9766\n",
            "Epoch [0/1], Step [1400/3236], Loss: 2.5806, Perplexity: 13.2056\n",
            "Epoch [0/1], Step [1500/3236], Loss: 2.4923, Perplexity: 12.0894\n",
            "Epoch [0/1], Step [1600/3236], Loss: 2.3363, Perplexity: 10.3428\n",
            "Epoch [0/1], Step [1700/3236], Loss: 2.3099, Perplexity: 10.0738\n",
            "Epoch [0/1], Step [1800/3236], Loss: 2.6100, Perplexity: 13.5987\n",
            "Epoch [0/1], Step [1900/3236], Loss: 2.3121, Perplexity: 10.0952\n",
            "Epoch [0/1], Step [2000/3236], Loss: 2.3588, Perplexity: 10.5788\n",
            "Epoch [0/1], Step [2100/3236], Loss: 2.3550, Perplexity: 10.5379\n",
            "Epoch [0/1], Step [2200/3236], Loss: 2.4449, Perplexity: 11.5291\n",
            "Epoch [0/1], Step [2300/3236], Loss: 2.4351, Perplexity: 11.4168\n",
            "Epoch [0/1], Step [2400/3236], Loss: 2.4679, Perplexity: 11.7975\n",
            "Epoch [0/1], Step [2500/3236], Loss: 2.4074, Perplexity: 11.1046\n",
            "Epoch [0/1], Step [2600/3236], Loss: 2.3659, Perplexity: 10.6532\n",
            "Epoch [0/1], Step [2700/3236], Loss: 2.1834, Perplexity: 8.8766\n",
            "Epoch [0/1], Step [2800/3236], Loss: 2.1896, Perplexity: 8.9316\n",
            "Epoch [0/1], Step [2900/3236], Loss: 2.2827, Perplexity: 9.8028\n",
            "Epoch [0/1], Step [3000/3236], Loss: 2.4587, Perplexity: 11.6900\n",
            "Epoch [0/1], Step [3100/3236], Loss: 2.2341, Perplexity: 9.3384\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eWsdM5_lOfEV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Uncomment the next cell and change image_path to generate caption for one image."
      ]
    },
    {
      "metadata": {
        "id": "9H0QK4tDQKPI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "d2f1d948-474b-4141-c85d-6b512c585c3e"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "!python sample.py --image='red_house.png' --encoder_path='vgg_models/encoder-1-1000.ckpt' --decoder_path='vgg_models/decoder-1-1000.ckpt' \n",
        "!python sample.py --image='red_house.png' --encoder_path='vgg_models/encoder-1-2000.ckpt' --decoder_path='vgg_models/decoder-1-2000.ckpt' \n",
        "!python sample.py --image='red_house.png' --encoder_path='vgg_models/encoder-1-3000.ckpt' --decoder_path='vgg_models/decoder-1-3000.ckpt' "
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> a man in a red shirt is sitting on a bench . <end>\n",
            "<start> a man is standing on a bench in front of a building . <end>\n",
            "<start> a man is sitting on a bench with a dog . <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6Ts0r5pjR_ae",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The following cells will compute bleu scores on the validation set.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "QAcbV9rmWWuD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# resize all validation images and store into resizedval2014\n",
        "\n",
        "!python resize.py --image_dir='data/val2014' --output_dir='data/resizedval2014'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e7aNIvUFTHSD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# change encoder_path and decoder_path to test the effect of different states of the models\n",
        "encoder_path = 'vgg_models/encoder-1-1000.ckpt'\n",
        "decoder_path = 'vgg_models/decoder-1-1000.ckpt'\n",
        "\n",
        "vocab_path = 'data/vocab.pkl'\n",
        "\n",
        "imagedir = 'data/resizedval2014/'\n",
        "caption_path = 'data/annotations/captions_val2014.json'\n",
        "\n",
        "embed_size = 256\n",
        "hidden_size = 512\n",
        "num_layers = 1\n",
        "\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np \n",
        "import pickle \n",
        "import os\n",
        "from torchvision import transforms \n",
        "from build_vocab import Vocabulary\n",
        "from model import EncoderCNN, DecoderRNN\n",
        "from PIL import Image\n",
        "\n",
        "from pycocotools.coco import COCO\n",
        "\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def load_image(image_path, transform=None):\n",
        "    image = Image.open(image_path)\n",
        "#     image = image.resize([224, 224], Image.LANCZOS)\n",
        "    \n",
        "    image = transform(image).unsqueeze(0)\n",
        "    return image\n",
        "  \n",
        "def Compute_bleu():\n",
        "  sum_bleu = 0\n",
        "  coco = COCO(caption_path)\n",
        "  ann_ids = list(coco.anns.keys())\n",
        "  print(len(ann_ids))\n",
        "  counter = 0\n",
        "  for ann_id in ann_ids:\n",
        "    # Image preprocessing\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(), \n",
        "        transforms.Normalize((0.485, 0.456, 0.406), \n",
        "                             (0.229, 0.224, 0.225))])\n",
        "    \n",
        "    # Load vocabulary wrapper\n",
        "    with open(vocab_path, 'rb') as f:\n",
        "        vocab = pickle.load(f)\n",
        "\n",
        "    # Build models\n",
        "    encoder = EncoderCNN(embed_size).eval()  # eval mode (batchnorm uses moving mean/variance)\n",
        "    decoder = DecoderRNN(embed_size, hidden_size, len(vocab), num_layers)\n",
        "    encoder = encoder.to(device)\n",
        "    decoder = decoder.to(device)\n",
        "\n",
        "    # Load the trained model parameters\n",
        "    encoder.load_state_dict(torch.load(encoder_path))\n",
        "    decoder.load_state_dict(torch.load(decoder_path))\n",
        "\n",
        "    # Prepare an image\n",
        "    img_id = coco.anns[ann_id]['image_id']\n",
        "    path = coco.loadImgs(img_id)[0]['file_name']\n",
        "    image_path = imagedir + path\n",
        "    \n",
        "    image = load_image(image_path, transform)\n",
        "    image_tensor = image.to(device)\n",
        "    \n",
        "    # Generate an caption from the image\n",
        "    feature = encoder(image_tensor)\n",
        "    sampled_ids = decoder.sample(feature)\n",
        "    sampled_ids = sampled_ids[0].cpu().numpy()          # (1, max_seq_length) -> (max_seq_length)\n",
        "    \n",
        "    # Convert word_ids to words\n",
        "    sampled_caption = []\n",
        "    for word_id in sampled_ids:\n",
        "        word = vocab.idx2word[word_id]\n",
        "        sampled_caption.append(word)\n",
        "        if word == '<end>':\n",
        "            break\n",
        "    sentence = ' '.join(sampled_caption)\n",
        "    \n",
        "    \n",
        "    # get reference captions\n",
        "    cap = coco.anns[ann_id]['caption']\n",
        "    tokens = nltk.tokenize.word_tokenize(str(cap).lower())\n",
        "    caption = []\n",
        "    caption.append('<start>')\n",
        "    caption.extend([token for token in tokens])\n",
        "    caption.append('<end>')\n",
        "    \n",
        "    references = [caption]\n",
        "    hypotheses = [sentence]\n",
        "#     print(references)\n",
        "    bleu = corpus_bleu(references, hypotheses, weights=(1.0, 0, 0, 0))\n",
        "    sum_bleu += bleu\n",
        "#     print(bleu)\n",
        "    counter += 1\n",
        "  \n",
        "    if counter % 10 == 0:\n",
        "      print('val size:', counter)\n",
        "      print('net bleu:', sum_bleu/counter)\n",
        "      print('Ex reference:', ' '.join(caption))\n",
        "      print('Ex hypothesis:', sentence)\n",
        "      print('Ex image:', image_path)\n",
        "      \n",
        "  print(sum_bleu/len(ann_ids))\n",
        "    \n",
        "    # Print out the image and the generated caption\n",
        "#     print (sentence)\n",
        "#     image = Image.open(image)\n",
        "#     plt.imshow(np.asarray(image))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b9I5LBWmgLAE",
        "colab_type": "code",
        "outputId": "8578ec7b-e46b-421c-ef52-fa3d56596cfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 829
        }
      },
      "cell_type": "code",
      "source": [
        "Compute_bleu()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.30s)\n",
            "creating index...\n",
            "index created!\n",
            "202654\n",
            "val size: 10\n",
            "net bleu: 0.3531404720435889\n",
            "Ex reference: <start> the bike has a clock as a tire . <end>\n",
            "Ex hypothesis: <start> a person sitting on a bench in a park . <end>\n",
            "Ex image: data/resizedval2014/COCO_val2014_000000203564.jpg\n",
            "val size: 20\n",
            "net bleu: 0.3363080518469441\n",
            "Ex reference: <start> two women waiting at a bench next to a street . <end>\n",
            "Ex hypothesis: <start> a man in a red shirt is riding a motorcycle . <end>\n",
            "Ex image: data/resizedval2014/COCO_val2014_000000517069.jpg\n",
            "val size: 30\n",
            "net bleu: 0.33739377647797725\n",
            "Ex reference: <start> city street with parked cars and a bench . <end>\n",
            "Ex hypothesis: <start> a motorcycle parked next to a car parked next to a building . <end>\n",
            "Ex image: data/resizedval2014/COCO_val2014_000000016977.jpg\n",
            "val size: 40\n",
            "net bleu: 0.3369011649695255\n",
            "Ex reference: <start> a beautiful dessert waiting to be shared by two people <end>\n",
            "Ex hypothesis: <start> a plate with a sandwich and a spoon on a table . <end>\n",
            "Ex image: data/resizedval2014/COCO_val2014_000000182417.jpg\n",
            "val size: 50\n",
            "net bleu: 0.34153015961591765\n",
            "Ex reference: <start> this is an advanced toilet with a sink and control panel . <end>\n",
            "Ex hypothesis: <start> a bathroom with a toilet and a sink . <end>\n",
            "Ex image: data/resizedval2014/COCO_val2014_000000331352.jpg\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-e3eecf32eed9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mCompute_bleu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-8dbdf3b9d2c5>\u001b[0m in \u001b[0;36mCompute_bleu\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m# Build models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoderCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# eval mode (batchnorm uses moving mean/variance)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecoderRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, embed_size, hidden_size, vocab_size, num_layers, max_seq_length)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;34m\"\"\"Set the hyper-parameters and build the layers.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDecoderRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_embeddings, embedding_dim, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse, _weight)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_weight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnum_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mreset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/init.py\u001b[0m in \u001b[0;36mnormal_\u001b[0;34m(tensor, mean, std)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \"\"\"\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}